\begin{thebibliography}{10}

\bibitem{al-shedivat2021federated}
Maruan Al-Shedivat, Jennifer Gillenwater, Eric Xing, and Afshin Rostamizadeh.
\newblock Federated learning via posterior averaging: A new perspective and practical algorithms.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{alam2022fedrolex}
Samiul Alam, Luyang Liu, Ming Yan, and Mi~Zhang.
\newblock Fedrolex: Model-heterogeneous federated learning with rolling sub-model extraction.
\newblock {\em Advances in neural information processing systems}, 35:29677--29690, 2022.

\bibitem{albrecht2016gdpr}
Jan~Philipp Albrecht.
\newblock How the gdpr will change the world.
\newblock {\em Eur. Data Prot. L. Rev.}, 2:287, 2016.

\bibitem{bengio2013STE}
Yoshua Bengio, Nicholas LÃ©onard, and Aaron Courville.
\newblock Estimating or propagating gradients through stochastic neurons for conditional computation, 2013.

\bibitem{beznosikov2021decentralized}
Aleksandr Beznosikov, Vadim Sushko, Abdurakhmon Sadiev, and Alexander Gasnikov.
\newblock Decentralized personalized federated min-max problems.
\newblock {\em arXiv preprint arXiv:2106.07289}, 2021.

\bibitem{borodich2021decentralized}
Ekaterina Borodich, Aleksandr Beznosikov, Abdurakhmon Sadiev, Vadim Sushko, Nikolay Savelyev, Martin Tak{\'a}{\v{c}}, and Alexander Gasnikov.
\newblock Decentralized personalized federated min-max problems.
\newblock {\em arXiv preprint arXiv:2106.07289}, 2021.

\bibitem{chezhegov2024local}
Savelii Chezhegov, Sergey Skorik, Nikolas Khachaturov, Danil Shalagin, Aram Avetisyan, Aleksandr Beznosikov, Martin Tak{\'a}{\v{c}}, Yaroslav Kholodov, and Alexander Gasnikov.
\newblock Local methods with adaptivity via scaling.
\newblock {\em arXiv preprint arXiv:2406.00846}, 2024.

\bibitem{choquette2021capc}
Christopher~A. Choquette-Choo, Natalie Dullerud, Adam Dziedzic, Yunxiang Zhang, Somesh Jha, Nicolas Papernot, and Xiao Wang.
\newblock {CaPC Learning: Confidential and Private Collaborative Learning}.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{diao2020heterofl}
Enmao Diao, Jie Ding, and Vahid Tarokh.
\newblock Heterofl: Computation and communication efficient federated learning for heterogeneous clients.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{gasanov22flix}
Elnur Gasanov, Ahmed Khaled, Samuel Horv\'ath, and Peter Richtarik.
\newblock Flix: A simple and communication-efficient alternative to local methods in federated learning.
\newblock In Gustau Camps-Valls, Francisco J.~R. Ruiz, and Isabel Valera, editors, {\em Proceedings of The 25th International Conference on Artificial Intelligence and Statistics}, volume 151 of {\em Proceedings of Machine Learning Research}, pages 11374--11421. PMLR, 28--30 Mar 2022.

\bibitem{guo2023federated}
Han Guo, Philip Greengard, Hongyi Wang, Andrew Gelman, Yoon Kim, and Eric Xing.
\newblock Federated learning as variational inference: A scalable expectation propagation approach.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{hanzely2023personalized}
Filip Hanzely, Boxin Zhao, and mladen kolar.
\newblock Personalized federated learning: A unified framework and universal optimization techniques.
\newblock {\em Transactions on Machine Learning Research}, 2023.

\bibitem{horvath2021fjord}
Samuel Horvath, Stefanos Laskaridis, Mario Almeida, Ilias Leontiadis, Stylianos Venieris, and Nicholas Lane.
\newblock Fjord: Fair and accurate federated learning under heterogeneous targets with ordered dropout.
\newblock {\em Advances in Neural Information Processing Systems}, 34:12876--12889, 2021.

\bibitem{isik2022sparse}
Berivan Isik, Francesco Pase, Deniz Gunduz, Tsachy Weissman, and Zorzi Michele.
\newblock Sparse random networks for communication-efficient federated learning.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{ji2021dynamic}
Shaoxiong Ji, Wenqi Jiang, Anwar Walid, and Xue Li.
\newblock Dynamic sampling and selective masking for communication-efficient federated learning.
\newblock {\em IEEE Intelligent Systems}, 37(2):27--34, 2021.

\bibitem{jiang2023fair}
Meirui Jiang, Holger~R Roth, Wenqi Li, Dong Yang, Can Zhao, Vishwesh Nath, Daguang Xu, Qi~Dou, and Ziyue Xu.
\newblock Fair federated medical image segmentation via client contribution estimation.
\newblock {\em arXiv preprint arXiv:2303.16520}, 2023.

\bibitem{jiang2022prunefl}
Yuang Jiang, Shiqiang Wang, Victor Valls, Bong~Jun Ko, Wei-Han Lee, Kin~K Leung, and Leandros Tassiulas.
\newblock Model pruning enables efficient federated learning on edge devices.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems}, 2022.

\bibitem{kairouz2021advances}
Peter Kairouz, H~Brendan McMahan, Brendan Avent, Aur{\'e}lien Bellet, Mehdi Bennis, Arjun~Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et~al.
\newblock Advances and open problems in federated learning.
\newblock {\em Foundations and trends{\textregistered} in machine learning}, 14(1--2):1--210, 2021.

\bibitem{karimireddy2020scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda~Theertha Suresh.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In {\em International conference on machine learning}, pages 5132--5143. PMLR, 2020.

\bibitem{khaled2019first}
Ahmed Khaled, Konstantin Mishchenko, and Peter Richt{\'a}rik.
\newblock First analysis of local gd on heterogeneous data.
\newblock In {\em International Workshop on Federated Learning for User Privacy and Data Confidentiality in Conjunction with NeurIPS}, 2019.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock {\em Toronto, ON, Canada}, 2009.

\bibitem{lecun1998mnist}
Yann LeCun.
\newblock The mnist database of handwritten digits.
\newblock {\em http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem{li2020lotteryfl}
Ang Li, Jingwei Sun, Binghui Wang, Lin Duan, Sicheng Li, Yiran Chen, and Hai Li.
\newblock Lotteryfl: Personalized and communication-efficient federated learning with lottery ticket hypothesis on non-iid datasets.
\newblock {\em arXiv preprint arXiv:2008.03371}, 2020.

\bibitem{li2021fedmask}
Ang Li, Jingwei Sun, Xiao Zeng, Mi~Zhang, Hai Li, and Yiran Chen.
\newblock Fedmask: Joint computation and communication-efficient personalized federated learning via heterogeneous masking.
\newblock In {\em Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems}, pages 42--55, 2021.

\bibitem{li2019rsa}
Liping Li, Wei Xu, Tianyi Chen, Georgios~B Giannakis, and Qing Ling.
\newblock Rsa: Byzantine-robust stochastic aggregation methods for distributed learning from heterogeneous datasets.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~33, pages 1544--1551, 2019.

\bibitem{li2020practical}
Qinbin Li, Bingsheng He, and Dawn Song.
\newblock Practical one-shot federated learning for cross-silo setting.
\newblock In Zhi-Hua Zhou, editor, {\em Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, {IJCAI-21}}, pages 1484--1490. International Joint Conferences on Artificial Intelligence Organization, 8 2021.
\newblock Main Track.

\bibitem{li2021ditto}
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith.
\newblock Ditto: Fair and robust federated learning through personalization.
\newblock In {\em International conference on machine learning}, pages 6357--6368. PMLR, 2021.

\bibitem{li2020federated}
Tian Li, Anit~Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
\newblock Federated optimization in heterogeneous networks.
\newblock In I.~Dhillon, D.~Papailiopoulos, and V.~Sze, editors, {\em Proceedings of Machine Learning and Systems}, volume~2, pages 429--450, 2020.

\bibitem{li2019convergence}
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.
\newblock On the convergence of fedavg on non-iid data.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{li2021fedbn}
Xiaoxiao Li, Meirui JIANG, Xiaofei Zhang, Michael Kamp, and Qi~Dou.
\newblock Fed{BN}: Federated learning on non-{IID} features via local batch normalization.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{lin2020ensemble}
Tao Lin, Lingjing Kong, Sebastian~U Stich, and Martin Jaggi.
\newblock Ensemble distillation for robust model fusion in federated learning.
\newblock {\em Advances in Neural Information Processing Systems}, 33:2351--2363, 2020.

\bibitem{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized data.
\newblock In {\em Artificial intelligence and statistics}, pages 1273--1282. PMLR, 2017.

\bibitem{mellor2021naswot}
Joe Mellor, Jack Turner, Amos Storkey, and Elliot~J Crowley.
\newblock Neural architecture search without training.
\newblock In {\em International conference on machine learning}, pages 7588--7598. PMLR, 2021.

\bibitem{mishchenko2022proxskip}
Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter Richt{\'a}rik.
\newblock Proxskip: Yes! local gradient steps provably lead to communication acceleration! finally!
\newblock In {\em International Conference on Machine Learning}, pages 15750--15769. PMLR, 2022.

\bibitem{mozaffari2021frl}
Hamid Mozaffari, Virat Shejwalkar, and Amir Houmansadr.
\newblock Frl: Federated rank learning.
\newblock {\em arXiv preprint arXiv:2110.04350}, 2021.

\bibitem{nader2020feddropoutadaptive}
Bouacida Nader, Hou Jiahui, Zang Hui, and Liu Xin.
\newblock Adaptive federated dropout: Improving communication efficiency and generalization for federated learning.
\newblock {\em arXiv preprint arXiv:2011.04050}, 2020.

\bibitem{rahimi2023evofed}
Mohammad~Mahdi Rahimi, Hasnain~Irshad Bhatti, Younghyun Park, Humaira Kousar, Do-Yeon Kim, and Jaekyun Moon.
\newblock Evofed: Leveraging evolutionary strategies for communication-efficient federated learning.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{sadiev2022decentralized}
Abdurakhmon Sadiev, Ekaterina Borodich, Aleksandr Beznosikov, Darina Dvinskikh, Saveliy Chezhegov, Rachael Tappenden, Martin Tak{\'a}{\v{c}}, and Alexander Gasnikov.
\newblock Decentralized personalized federated learning: Lower bounds and optimal algorithm for all personalization modes.
\newblock {\em EURO Journal on Computational Optimization}, 10:100041, 2022.

\bibitem{tastan2024redefining}
Nurbek Tastan, Samar Fares, Toluwani Aremu, Samuel HorvÃ¡th, and Karthik Nandakumar.
\newblock Redefining contributions: Shapley-driven federated learning.
\newblock In Kate Larson, editor, {\em Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, {IJCAI-24}}, pages 5009--5017. International Joint Conferences on Artificial Intelligence Organization, 8 2024.
\newblock Main Track.

\bibitem{tastan2023capride}
Nurbek Tastan and Karthik Nandakumar.
\newblock Capride learning: Confidential and private decentralized learning based on encryption-friendly distillation loss.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2023.

\bibitem{tupitsa2024federated}
Nazarii Tupitsa, Samuel Horv{\'a}th, Martin Tak{\'a}{\v{c}}, and Eduard Gorbunov.
\newblock Federated learning can find friends that are advantageous.
\newblock {\em arXiv preprint arXiv:2402.05050}, 2024.

\bibitem{ullah2023private}
Enayat Ullah, Christopher~A Choquette-Choo, Peter Kairouz, and Sewoong Oh.
\newblock Private federated learning with autotuned compression.
\newblock In {\em International Conference on Machine Learning}, pages 34668--34708. PMLR, 2023.

\bibitem{wang2020federated}
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni.
\newblock Federated learning with matched averaging.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{wang2020tackling}
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H~Vincent Poor.
\newblock Tackling the objective inconsistency problem in heterogeneous federated optimization.
\newblock {\em Advances in neural information processing systems}, 33:7611--7623, 2020.

\bibitem{wang2019matcha}
Jianyu Wang, Anit~Kumar Sahu, Zhouyi Yang, Gauri Joshi, and Soummya Kar.
\newblock Matcha: Speeding up decentralized sgd via matching decomposition sampling.
\newblock In {\em 2019 Sixth Indian Control Conference (ICC)}, pages 299--300. IEEE, 2019.

\bibitem{xu2021gradient}
Xinyi Xu, Lingjuan Lyu, Xingjun Ma, Chenglin Miao, Chuan~Sheng Foo, and Bryan Kian~Hsiang Low.
\newblock Gradient driven rewards to guarantee fairness in collaborative machine learning.
\newblock {\em Advances in Neural Information Processing Systems}, 34:16104--16117, 2021.

\bibitem{medmnistv2}
Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni.
\newblock Medmnist v2-a large-scale lightweight benchmark for 2d and 3d biomedical image classification.
\newblock {\em Scientific Data}, 10(1):41, 2023.

\bibitem{yoon2021fedweit}
Jaehong Yoon, Wonyong Jeong, Giwoong Lee, Eunho Yang, and Sung~Ju Hwang.
\newblock Federated continual learning with weighted inter-client transfer.
\newblock In {\em International Conference on Machine Learning}, 2021.

\bibitem{yuan2022distributed}
Binhang Yuan, Cameron~R Wolfe, Chen Dun, Yuxin Tang, Anastasios Kyrillidis, and Chris Jermaine.
\newblock Distributed learning of fully connected neural networks using independent subnet training.
\newblock {\em Proceedings of the VLDB Endowment}, 15(8), 2022.

\bibitem{yurochkin2019bayesian}
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni.
\newblock Bayesian nonparametric federated learning of neural networks.
\newblock In {\em International conference on machine learning}, pages 7252--7261. PMLR, 2019.

\end{thebibliography}
