\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers}{natbib}
% before loading neurips_2024


% ready for submission
\usepackage[preprint]{neurips_2024}


\input{math_commands.tex}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
% \usepackage[numbers]{natbib}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}

\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{booktabs, caption, makecell}
\usepackage{threeparttable}
\usepackage{progressbar}
\usepackage{wrapfig}

\usepackage{multirow}
\usepackage{xcolor,colortbl}
\usepackage{siunitx} % For aligning at decimal points

\usepackage{subcaption}

\definecolor{lightred}{rgb}{1,0.8,0.8}
\definecolor{lightgreen}{rgb}{0.8,1,0.8}
\definecolor{lightblue}{rgb}{0.88,0.96,1} 

\newcommand{\blue}{\color{blue}}
\usepackage{comment}


\title{FedPeWS: Personalized Warmup via Subnetworks \\ for Enhanced Heterogeneous Federated Learning} 



\author{%
  Nurbek Tastan\\
  MBZUAI\\ 
  \texttt{nurbek.tastan@mbzuai.ac.ae} \\ 
  \And
  Samuel Horv{\'a}th \\
  MBZUAI \\
  \texttt{samuel.horvath@mbzuai.ac.ae} \\
  \AND
  \hspace{1.05em} Martin Tak{\'a}{\v{c}} \\
  \hspace{1.05em} MBZUAI \\
  \hspace{1.05em} \texttt{martin.takac@mbzuai.ac.ae} \\
  \And
  \hspace{0.1em} Karthik Nandakumar \\
  \hspace{0.1em} MBZUAI \\
  \hspace{0.1em} \texttt{karthik.nandakumar@mbzuai.ac.ae} \\
}

\begin{document}

\maketitle


\begin{abstract}
  Statistical data heterogeneity is a significant barrier to convergence in federated learning (FL). While prior work has advanced heterogeneous FL through better optimization objectives, these methods fall short when there is \textit{extreme} data heterogeneity among collaborating participants. We hypothesize that convergence under extreme data heterogeneity is primarily hindered due to the aggregation of conflicting updates from the participants in the initial collaboration rounds. To overcome this problem, we propose a warmup phase where each participant learns a personalized mask and updates only a subnetwork of the full model. This \textit{personalized warmup} allows the participants to focus initially on learning specific \textit{subnetworks} tailored to the heterogeneity of their data. After the warmup phase, the participants revert to standard federated optimization, where all parameters are communicated. We empirically demonstrate that the proposed personalized warmup via subnetworks (\texttt{FedPeWS}) approach improves accuracy and convergence speed over standard federated optimization methods. 
\end{abstract}


\section{Introduction} 
\label{section: intro}

Federated learning (FL) is a distributed learning paradigm where participants collaboratively train a global model by performing local training on their data and periodically sharing local updates with the server. The server, in turn, aggregates the local updates to obtain the global model, which is then transmitted to the participants for the next round of training \citep{mcmahan2017communication}. While FL preserves data confidentiality by avoiding collating participant data at the server, \textit{statistical heterogeneity} between local data distributions is a significant challenge in FL \citep{kairouz2021advances}. Several attempts have been made to tackle heterogeneity via federated optimization algorithms \citep{wang2019matcha, khaled2019first, li2019convergence, li2020federated, karimireddy2020scaffold,tupitsa2024federated,sadiev2022decentralized,beznosikov2021decentralized}, dropout \citep{horvath2021fjord, alam2022fedrolex}, and batch normalization \citep{li2021fedbn}. 

Consider the scenario where multiple hospitals collaborate to learn a medical image classification model that works across imaging modalities and organs, where the data from each hospital pertains to a different modality (e.g., histopathology, CT, X-ray, ultrasound, etc.) and/or organ (e.g., brain, kidney, colon, etc.). Most of the existing heterogeneous FL algorithms fail when there is such \textit{extreme} data heterogeneity among collaborating participants, especially when the model is learned from scratch (with random initialization). The main reason for this failure is the high degree of conflicts between the local updates during the initial collaboration rounds. While enforcing a strong regularization constraint on the local updates (\cite{li2020federated}) can partially alleviate this problem, it dramatically slows down local learning and hence, convergence speed. 

In this work, we explore an alternate approach to minimize the initial conflicts between heterogeneous participants by allowing participants in FL to initially train a partial subnetwork using only their local datasets. This warmup phase enables the participants to focus first on learning their local data well before engaging in broader collaboration. Thus, our proposed approach can be summarized as follows (see Figure \ref{fig: main-figure}). Initially, each participant uses a personalized binary mask tailored to their data distributions, allowing them to first learn their local data distributions and optimize their local (sparse) models. During this warmup phase, participants transmit only their masked updates to the server, and this process continues for a certain number of collaboration rounds. At the end of the warmup phase, the participants switch to standard federated optimization methods for subsequent collaboration. Our contributions are as follows:

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{images/main_scheme_v4.pdf} 
    \caption{Conceptual illustration of training personalized subnetworks in federated learning.}
    \label{fig: main-figure}
\end{figure}

\begin{enumerate}
    \item We introduce a novel concept in federated learning, termed as \textit{personalized warmup via subnetworks} (\texttt{FedPeWS}), which helps the global model to generalize to a better solution in fewer communication rounds. This is achieved through a neuron-level personalized masking strategy that is compatible with other FL optimization methods. 
    \item We propose an algorithm to \textit{identify suitable subnetworks} (subset of neurons) for each participant by simultaneously learning the personalized masks and parameter updates. The proposed algorithm does not make any assumptions regarding the data distributions and incorporates a \textit{mask diversity loss} to improve the coverage of all neurons in the global model. 
    \item For simple cases involving a small number of participants with known data distributions, we show that it is possible to skip the mask learning step and use fixed masks (that partition the network) determined by the server. We refer to this variant as \texttt{FedPeWS-Fixed}. 
    \item We empirically demonstrate the efficacy of the \texttt{FedPeWS} approach under both extreme non-i.i.d. and i.i.d. data scenarios using three datasets: a custom synthetic dataset, a combination of MNIST and CIFAR-10 datasets, and a combination of three distinct medical datasets (PathMNIST, OCTMNIST and TissueMNIST). 
\end{enumerate} 

\section{Related Work} 

\paragraph{Collaborative Learning.} FL is a distributed learning paradigm that addresses data confidentiality concerns \citep{kairouz2021advances}, particularly in environments where data can not be centralized due to regulatory or practical reasons \citep{albrecht2016gdpr}. One of the seminal FL algorithms, FedAvg \citep{mcmahan2017communication}, involves participants training models locally on their data and periodically transmitting their model parameters to a central server. The server averages these parameters to update the global model, which is then redistributed to the participants for further local refinement. FedAvg has inspired a plethora of variants and extensions aimed at enhancing performance \citep{karimireddy2020scaffold, li2020federated, mishchenko2022proxskip}, scalability \citep{guo2023federated, al-shedivat2021federated}, communication efficiency \citep{ullah2023private, rahimi2023evofed, isik2022sparse}, privacy/confidentiality \citep{tastan2023capride, choquette2021capc, ullah2023private}, robustness \citep{li2019rsa}, and fairness \citep{xu2021gradient, jiang2023fair, tastan2024redefining}. For example, strategies such as weighted averaging or adaptive aggregation have been proposed to accommodate the non-i.i.d. nature of distributed data sources $-$ a scenario where data is not identically distributed across all participants, which can significantly hinder model performance \citep{li2020federated, wang2020tackling, karimireddy2020scaffold, li2021fedbn, wang2020federated, chezhegov2024local}. Specifically, FedProx \citep{li2020federated} addresses data heterogeneity by integrating a proximal term into the FedAvg framework. There is also a body of work that focuses on addressing the heterogeneity problem through personalization-based approaches, utilizing local-centric objectives \citep{gasanov22flix, hanzely2023personalized, yoon2021fedweit,sadiev2022decentralized,borodich2021decentralized, li2021ditto}. 

\paragraph{Independent Subnet Training.} Independent subnet training (IST) is a variant of distributed learning that focuses on enhancing model personalization and reducing communication overhead by training separate subnetworks for different participants \citep{yuan2022distributed}. IST distributes neurons of a fully connected neural network disjointly across different participants, forming a group of subnets. Then, each of these subnets is trained independently for one or more local SGD steps before synchronization. In every round, after broadcasting the server weights, each participant gets updated neurons to focus on, and the local subnet training continues. This approach led to different works along the line of using subnetwork training for efficiency \citep{horvath2021fjord, jiang2022prunefl, diao2020heterofl, nader2020feddropoutadaptive, alam2022fedrolex, li2021fedmask, mozaffari2021frl} in FL. In our work, we adopt IST's core principle of selecting neurons rather than focusing on weight values, which in turn narrows the search space. A key distinction between our method and IST lies in how the neurons are selected and the necessity of covering all neurons. Whilst IST typically involves random sampling of masks in each training round by the server and full coverage of neurons, we do not randomly sample neurons; instead, we use a learnable mask for each participant that is trained along with the parameters, and we relax the assumption of full coverage of neurons. 

\paragraph{Finding Subnetworks in FL.} Another relevant idea is the Lottery Ticket Hypothesis (LTH) \citep{frankle2018lottery}, which attempts to identify subnetworks within a larger network. LTH is a model personalization technique, which focuses on sparsifying the network to create a smaller-scale version that improves per-round communication efficiency. In contrast to LTH, our method is directed towards training a shared global model and simultaneously improving convergence speed (reducing number of communication rounds). After LTH, there has been a growing interest in finding sparse and trainable networks at initialization \citep{mellor2021naswot, ji2021dynamic, li2020lotteryfl}. Recently, in \citep{isik2022sparse}, sparse networks were found inside the main model to increase communication efficiency in FL. The proposed FedPM method focuses on finding a subnetwork by freezing the model weights and training for masks on a weight level, in contrast to IST, which works on a neuron level. FedPM utilizes the sigmoid function to obtain probability values from unbounded mask scores and then uses Bernoulli sampling to obtain binary masks. We use a similar approach in our \texttt{FedPeWS} algorithm to learn the neuron-level personalized masks. 


\section{Preliminaries}
Our goal is to minimize a sum-structured federated learning optimization objective: 
\begin{equation}
    \begin{split}
        x^{\star} \leftarrow  \argmin_{x \in \mathbb{R}^d} \Bigg[ f(x) \coloneqq \frac{1}{N} \sum_{i=1}^N f_i(x) \Bigg], 
        \label{eq: optimization-problem}
    \end{split}
\end{equation}
where the components $f_i: \mathbb{R}^d \rightarrow \mathbb{R}$ are distributed among $N$ local participants and are expressed in a stochastic format as $f_i(x) \coloneqq \mathbb{E}_{\xi \sim \mathcal{D}_i}\big[F_i(x, \xi)\big]$. Here, $\mathcal{D}_i$ represents the distribution of $\xi$ at participant $i \in [N] \coloneqq \{1,\ldots,N\}$. This problem encapsulates standard empirical risk minimization as a particular case when each $\mathcal{D}_i$ is represented by a finite set of $n_i$ elements, i.e., $\xi_i = \{\xi_i^1, \ldots, \xi_i^{n_i} \}$. In such cases, $f_i$ simplifies to $f_i(x,\xi_i) = \frac{1}{n_i} \sum_{j=1}^{n_i} F_i(x,\xi_i^j)$. Our approach does not impose restrictive assumptions on the data distribution $\mathcal{D}_i$. In fact, we specifically focus on the extreme heterogeneous (non-i.i.d.) setting, where $\mathcal{D}_i \neq \mathcal{D}_{i^{'}}, \forall ~ i \neq i^{'}$ and the \textit{local optimal solution} $x_i^{\star}\leftarrow \argmin_{x \in \mathbb{R}^d} f_i(x)$ might significantly differ from the global minimizer of the objective function in Equation \ref{eq: optimization-problem}. 

We are especially interested in the supervised classification task and let $\mathcal{M}_x: \mathcal{Z} \rightarrow \mathcal{Y}$ be a classifier parameterized by $x$. Here, $\mathcal{Z} \subseteq \mathbb{R}^D$ and $\mathcal{Y} = \{1, 2, \ldots, M\}$ denote the input and label spaces, respectively, $D$ is the input dimensionality, $M$ is the number of classes, and $d$ represents the number of parameters in the model $\mathcal{M}$. We set $F_i(x,\xi_i^j) = \mathcal{L}(\mathcal{M}_x(\mathbf{z}_i^j), y_i^j)$, where $\mathcal{L}$ is an appropriate loss function and $\xi_i^j \coloneq (\mathbf{z}_i^j, y_i^j)$ is a labeled training sample such that $\mathbf{z}_i^j \in \mathcal{Z}$ and $y_i^j \in \mathcal{Y}$. Furthermore, we mainly focus on the cross-silo FL setting ($N$ is small). 

\paragraph{Federated Averaging (FedAvg).} A common approach for solving Equation \ref{eq: optimization-problem} in the distributed setting is FedAvg \citep{mcmahan2017communication}. This algorithm involves the participants performing $K$ local steps of stochastic gradient descent (SGD) and communicating with the server over $T$ communication rounds. The server initializes the global model with $x_g^0$ and broadcasts it to all participants, which is then used to initialize the local models, i.e., $x_i^{1,0} = x_g^0$. In each communication round, the updates from the participants are averaged on the server and sent back to all participants. For a local step $k \in [K]$, communication round $t \in [T]$, and participant $i \in [N]$, the local and global iterates are updated as: 
\begin{equation}
    x_{i}^{t,k} = x_{i}^{t,k-1} - \eta_{\ell} \nabla f_i\big(x_i^{t,k-1}, \xi_i^{t,k-1}\big), \quad x_{i}^{t} = x_{i}^{t,K}, \text{ and  } x_g^{t} = x_g^{t-1} - \eta_g (x_g^{t-1} - \frac{1}{N} \sum_{i=1}^N x_{i}^{t}) , 
\end{equation}
\noindent where $\eta_{\ell}$ and $\eta_{g}$ are the local and global learning rates, respectively. The server then broadcasts the updated global model $x_g^{t}$ to all participants, which is then used to reinitialize the local models as $x_i^{t+1,0} = x_g^{t}$. 

In the FedAvg algorithm, the number of communication rounds necessary to achieve a certain precision is directly proportional to the heterogeneity measure \citep{li2019convergence}. Notably, this relationship holds true in convex settings; however, in non-convex scenarios, the algorithm may either not converge or may converge to a suboptimal solution. Stemming from this observation, our objective is to reduce the number of requisite communication rounds to achieve convergence, while simultaneously achieving a better solution. 

\input{algorithm-fedpews}

\section{Proposed FedPeWS Method}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/fedpews_V2-1200.pdf}
    \caption{Illustration of the proposed \texttt{FedPeWS} algorithm for two participants, which aggregates partial subnetworks ($x_i^t \odot m_i^t$) during the warmup phase to obtain a shared global model $x_g^t$. Here, $x_i^t$ and $m_i^t$ denote the local model and personalized mask of the $i^{\text{th}}$ participant in the $t^{\text{th}}$ round.}
    \label{fig: fedpews-masked-aggregation}
\end{figure}

The core idea of the proposed \texttt{FedPeWS} method is to allow participants to learn only a personalized subnetwork (a subset of parameters) instead of the entire network (all parameters) during the initial warmup phase. Let $m_i \in \{0,1\}^d$ be a binary mask vector denoting the set of parameters that are learned by participant $i$, $i \in [N]$. Note that $m_i(\ell) = 1$ indicates that the $\ell^{\textrm{th}}$ element of $x_i$ is selected for learning (value $0$ indicates non-selection), $\ell \in [d]$. Thus, during the warmup phase, the objective in \texttt{FedPeWS} is to learn the parameters $x$ along with the personalized masks $m_i$, i.e., 

\begin{equation}
    \label{eq: mask learning}
    % \textstyle
    \min_{x, \{m_i\}_{i\in[N]}} \frac{1}{N} \sum_{i=1}^N f_i(x \odot m_i),
\end{equation}

\noindent $\odot$ denotes element-wise multiplication. Note that $\mathcal{M}_{x \odot m_i}$ denotes the personalized subnetwork of participant $i$. When personalized masks are employed, the update rules can be modified as:

\begin{equation}
    x_{i}^{t,k} = x_{i}^{t,k-1} - \eta_{\ell} \nabla f_i\big(x_i^{t,k-1} \odot m_i^t, \xi_i^{t,k-1}\big) \; \text{and} \; x_g^{t} = x_g^{t-1} - \eta_g (x_g^{t-1} - \frac{\sum_{i \in [N]}x_{i}^{t} \odot m_i^t}{\sum_{i \in [N]}m_i^t}).
\end{equation}

The obvious questions regarding the \texttt{FedPeWS} method are: (i) how to learn these personalized masks $m_i$? and (ii) what should be the length of the warmup period?

\textbf{Identification of personalized subnetworks}: It is not straightforward to directly optimize for the personalized binary (discrete) masks $m_i$ in \Eqref{eq: mask learning}. Hence, we make the following design choices. Firstly, personalized masks are learned at the neuron-level and then expanded to the parameter-level. Following IST \citep{yuan2022distributed}, masks are specifically applied only to the hidden layer neurons, while the head and tail neurons remain unaffected. However, unlike IST, the neuron-level masks are not randomly selected in each collaboration round. Instead, we learn real-valued personalized neuron-level mask score vectors $s_i \in \mathbb{R}^h$, which in turn can be used to generate the binary masks. Here, $h$ denotes the number of hidden neurons in the classifier $\mathcal{M}$ and $h \ll d$. A higher value of element $s_i(\ell)$, $\ell \in [h]$, indicates that the $\ell^{\textrm{th}}$ neuron is more likely to be selected by participant $i$.  Let $\mathcal{G}:\mathbb{R}^h \rightarrow \{0,1\}^d$ be the mask generation function that generates the binary parameter-level masks $m_i$ from neuron-level mask score vectors $s_i$, i.e., $m_i = \mathcal{G}(s_i)$. $\mathcal{G}$ consists of three steps. Firstly, we convert $s_i$ into probabilities by applying a sigmoid function, i.e., $\theta_i = \sigma(s_i)$, where $\theta_i \in [0,1]^h$ is the mask probability vector and $\sigma$ is the sigmoid function. Next, binary neuron masks $\tilde{m}_i$ are obtained by sampling from a Bernoulli distribution with parameter $\theta_i$, i.e., $\tilde{m}_i(\ell) \sim Bernoulli(\theta_i(\ell))$, $\forall~\ell \in [h]$. Finally, these binary neuron masks can be directly mapped to the binary parameter-level mask $m_i$, i.e., if a neuron is selected, all the weights associated with the selected neuron are also selected. Thus, \Eqref{eq: mask learning} can be reparameterized as:
\begin{equation}
    \label{eq: mask score learning}
    % \textstyle
    \min_{x, \{s_i\}_{i\in[N]}} \frac{1}{N} \sum_{i=1}^N f_i(x \odot \mathcal{G}(s_i)).
\end{equation}

The above equation can be optimized alternatively for the mask score vectors $s_i$ and the parameters $x$. The participants first optimize for the mask scores while the model parameters $x_{i}^{t,k}$ are frozen (Procedure I), and then switch to optimizing the model parameters while freezing the mask scores (Procedure II). In the mask training step (Procedure I), the optimization objective is defined as: 
\begin{equation}
    \mathcal{L}_s = f_i\Big(x_i^{t,k} \odot \mathcal{G}\left(s_i^{t,k}\right), \xi_i^{t,k-1}\Big) - \lambda \|\sigma\left(s_{i}^{t,k}\right) - \theta_{g \backslash \{i\}}^{t}\|_2^2; \qquad s_{i}^{t,k+1} \gets s_{i}^{t,k} - \eta_s \nabla_{s} \mathcal{L}_s, 
    \label{eq: mask-objective-func}
\end{equation}

where $\nabla_{s}$ indicates that the gradient is w.r.t. mask score vector $s$, $\eta_s$ is the local learning rate for updating $s$, $\theta_g^t$ is the global mask probability at round $t$, $\theta_{g \backslash \{i\}}^t$ is the global mask probability excluding the probability mask of the current participant $i$, and $\lambda$ is the weight assigned to the mask diversity measure (second term). It is important to note that the personalized masks may not cover all neurons in the network. Maximizing the mask diversity measure encourages personalized masks to deviate as much as possible from the global mask, which facilitates better coverage of all the neurons in the global model. The diversity measure has an upper bound due to the sigmoid function: 
\begin{equation}
    \|\sigma\left(s_{i}^{t,k}\right) - \theta_{g \backslash \{i\}}^{t}\|_2^2 \leq h. 
\end{equation}
\noindent Given the difficulty in calculating $\nabla_s \mathcal{L}_s$ directly due to the discrete nature of Bernoulli sampling, we employ the straight-through estimator (STE) \citep{bengio2013STE} to approximate the gradients, which does not compute the gradient of the given function and passes on the incoming gradient as if the function was an identity function.

During Procedure II, the optimization function for the model weights is expressed as: 
\begin{equation}
    \mathcal{L}_x = f_i\Big(x_i^{t,k} \odot \mathcal{G}\left(s_i^{t,k}\right), \xi_i^{t,k-1}\Big); \qquad x_{i}^{t,k+1} \gets x_{i}^{t,k} - \eta_{\ell} \nabla_{x} \mathcal{L}_x,
\end{equation}

where $\nabla_{x}$ indicates that the gradient is w.r.t. weights $x$. The \texttt{FedPeWS} algorithm alternates between these two procedures for $W$ rounds, where $W$ is the number of warmup rounds. 
At this point, the warmup stops and the participants switch to standard training for $(T-W)$ collaboration rounds. 
This approach ensures that each participant effectively contributes to the FL process while also tailoring the learning to their specific data distributions. The number of warmup rounds $W$ (or the proportion of warmup rounds $\tau = \frac{W}{T}$) is a key hyperparameter of the \texttt{FedPeWS} algorithm, along with the weight $\lambda$ assigned to the mask diversity loss. While it would be ideal to have a principled method to select these hyperparameters, we use a grid search to tune them, which is currently a limitation.

\textbf{Use of fixed subnetworks}: When the number of participants is small and the data distributions of the participants are known apriori, the server can partition the full model into subnetworks of the same depth and assign a fixed subnetwork to each participant, i.e., $m_i^t = m_i$, $\forall~ t \in [W]$. Participants transmit only the masked updates back to the server during warmup, which then aggregates these masked parameters and redistributes them in their masked form. For the sake of utility, the server can design personalized masks such that the union of these masks covers all the neurons. This variant of \texttt{FedPeWS} is referred to as \texttt{FedPeWS-Fixed} and follows the same algorithm in \Algref{algorithm: adaptivesubnet}, except for the omission of the highlighted (green) steps. 


\section{Experiments and Results}
\label{section: exp-and-results}
\subsection{Datasets and Network Architecture} 
\label{section: datasets}

\paragraph{Synthetic Dataset.} To effectively evaluate the performance of the proposed algorithm, we generated a custom synthetic dataset that simulates the extreme non i.i.d. scenario. This dataset encompasses four classes, each characterized by four 2D clusters determined by specific centers and covariance matrices. Note that the clusters from different classes interleave each other as shown in \Figref{fig:synthetic-data}.  For this dataset, we utilize a neural network consisting of five fully-connected (FC) layers, each followed by ReLU activation functions, except the last layer. To enhance the dataset complexity and aid FC network learning, we transform these 2D points into 5D space using the transformation $[x, y, x^2, y^2, xy]$, based on their $(x,y)$ coordinates. We generate two versions of this dataset, \textbf{Synthetic-32K} and \textbf{Synthetic-3.2K}, depending on the number of data points in the training set. The former has $32000$ samples, with each class containing $8000$ data points, while the latter has ten times fewer data points. 

\begin{wrapfigure}{r}{0.56\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/synthetic-data.pdf} 
    \caption{Samples from the custom synthetic dataset.}
    \label{fig:synthetic-data}
\end{wrapfigure}

\paragraph{CIFAR-MNIST.} We integrate two distinct datasets, CIFAR-10 \citep{krizhevsky2009learning} and MNIST \citep{lecun1998mnist}, to explore how different clients might adapt when faced with disparate data sources. CIFAR-10 comprises of $32 \times 32$ pixel images categorized into 10 object classes. MNIST, typically featuring $28 \times 28$ pixel images across 10 digit classes, is upscaled to $32 \times 32$ pixel to standardize dimensions with CIFAR-10. We compile a balanced dataset by randomly selecting 400 samples from each class for the training set and 200 samples for the test set from the combined pool of 20 classes. This setup aims to simulate a FL environment where multiple clients handle significantly varied data types. For this dataset, we employ a convolutional neural network comprising four convolutional layers, each having a kernel size of 3 and padding of 1, followed by max pooling. This is succeeded by a fully connected layer. This architecture was used because of its simplicity and widespread use in the literature \citep{yuan2022distributed, isik2022sparse}. 

\paragraph{\{Path-OCT-Tissue\}MNIST.} We amalgamate three distinct medical datasets: PathMNIST, OCTMNIST, and TissueMNIST \citep{medmnistv2}, to develop a universal medical prognosis model capable of recognizing various tasks using a single model. The datasets contain 9, 4, and 8 classes, respectively, totaling to 21 classes. For this dataset, we utilized the same architecture and training details described in the CIFAR-MNIST dataset. 


\subsection{Experimental Setup} 
\label{subsection: exp-setup}

\paragraph{Dataset partitioning.} For scenarios with a smaller number of collaborators ($N={2, 3, 4}$), we manually partition the training dataset to tailor the data distribution to specific participants. In the $N=2$ scenario, we partition as follows: (i) For the Synthetic dataset, encompassing both Synthetic-32K and Synthetic-3.2K, even-numbered classes are assigned to participant 1, while odd-numbered classes are allocated to participant 2. (ii) For the CIFAR-MNIST combination, all CIFAR-10 samples are assigned to participant 1, with MNIST samples allocated to participant 2. In the $N=3$ scenario, the \{Path-OCT-Tissue\}MNIST dataset is partitioned into three splits corresponding to the individual datasets, with PathMNIST assigned to participant 1, OCTMNIST to participant 2, and TissueMNIST to participant 3. For the $N=4$ scenario, the synthetic dataset is divided so that each class is exclusively allocated to one of the four participants. 

For scenarios with a larger number of participants ($N \geq 10$), we employ a Dirichlet distribution to partition the training set. This approach utilizes a concentration parameter $\alpha$ to simulate both homogeneous and heterogeneous data distributions \citep{yurochkin2019bayesian, li2020practical, lin2020ensemble, wang2020federated}. We experiment with various values of $\alpha$, specifically $\alpha \in \{0.1, 0.5, 1.0, 2.0, 5.0\}$, to explore the effects of dataset heterogeneity (lower $\alpha$ values) and homogeneity (higher $\alpha$ values) on the model performance. This methodological diversity allows us to comprehensively assess our approach under varying data conditions. Results for large $N$ ($> 100$) are reported in the appendix.  


\paragraph{Training details.} 
In federated optimization, we primarily benchmark against the FedAvg algorithm \citep{mcmahan2017communication}, a standard approach in federated learning. However, our algorithm is designed to be versatile, functioning as a `plug-and-play' solution that is compatible with various other optimizers. To demonstrate this adaptability, we also conduct experiments using FedProx \citep{li2020federated}, showcasing our method's capabilities across different optimization frameworks. For our experiments, we fix the local learning rate $\eta_{\ell}=0.001$ in the Synthetic-32K dataset case, and we set $\eta_{\ell}=0.01$ for other experiments. Also, the mask learning rate is fixed $\eta_s=0.1$. Furthermore, we vary the global learning rate $\eta_g \in \{0.1, 0.25, 0.5, 1.0\}$ to observe the differences in optimization behavior between the baseline and our proposed methods. Additionally, we employ two distinct batch sizes $\{32, 8\}$ for Synthetic-32K and Synthetic-3.2K, respectively. For experiments involving the CIFAR-MNIST and \{Path-OCT-Tissue\}MNIST datasets, we standardize the batch size to 64. 
We conduct our experiments on NVIDIA RTX A6000 GPUs on an internal cluster server, with each run utilizing a single GPU. The execution time for each run is capped at less than an hour, which indicates the maximum execution time rather than the average. All results are averaged over three independent runs and the average accuracy is reported on the global test dataset. 


\input{table-synth-comm-efficiency}

\begin{figure}[t] 
    \centering
    \includegraphics[width=\linewidth]{images/comb-synth-4-line-v2.pdf}
    \caption{Results of the experiments on Synthetic-\{32, 3.2\}K datasets with batch sizes \{32, 8\}, with different global learning rates $\eta_g \in \{1.0, 0.5, 0.25, 0.1\}$ and communication rounds $T \in \{200, 250, 400, 500\}$. Refer to Table \ref{table: convergence-table} for the corresponding numbers. In all the above scenarios, \texttt{FedPeWS} converges faster to a better solution compared to FedAvg.}
    \label{fig: synthetic-all}
\end{figure}


\subsection{Experimental Results}
\label{subsection: exp-results}


\begin{wrapfigure}{r}{0.51\textwidth}
    \vspace{-2em}
    \centering
    \includegraphics[width=\linewidth]{images/synth-n4-acc-loss-suptitle-v2.pdf}
    \caption{Visualization of validation accuracy and loss on the Synthetic-32K dataset with $N=4$ participants and a global learning rate $\eta_g=1.0$.} 
    \label{fig: n4-synthetic}
    \vspace{-1.0em}
\end{wrapfigure} 

Our experimental analysis focuses on assessing the performance of our proposed \texttt{FedPeWS} algorithm within the FL framework. The key findings from our studies are as follows: (i) The \texttt{FedPeWS} approach demonstrates a significant reduction in the number of communication rounds required to achieve target accuracy while also enhancing the final accuracy post-convergence. (ii) The \texttt{FedPeWS} algorithm is robust across different levels of data heterogeneity. (iii) In scenarios where full knowledge of the participant data distributions is available, the server can employ the \texttt{FedPeWS-Fixed} method (Figure \ref{fig: cifar-mnist-pot-accuracy-heatmap}). While the \texttt{FedPeWS-Fixed} variant shows competitive effectiveness comparable to our primary \texttt{FedPeWS} algorithm, the latter offers broader applicability in real-world settings.

\paragraph{Improved communication efficiency and accuracy.} We initially report the required number of communication rounds to reach the target accuracy and the final accuracy after $T$ communication rounds for the synthetic dataset in \Tabref{table: convergence-table}. The results underscore that the incorporation of a personalized warmup phase in a federated setup significantly reduces the required number of communication rounds across all tested scenarios. Notably, in specific instances, such as with the Synthetic-32K dataset and $\eta_g=0.25$, the conventional FedAvg algorithm does not meet the target accuracy within the $T$ communication rounds. Conversely, in scenarios where $\eta_g \in \{0.25, 0.1\}$, FedAvg only achieves the target accuracy in one of the seeds, exhibiting suboptimal performance in the other two runs. From \Figref{fig: synthetic-all}, it is evident that our proposed \texttt{FedPeWS} algorithm surpasses FedAvg in both communication efficiency and accuracy. 

We also consider a more extreme data heterogeneity scenario with $N=4$ participants, depicted in Figure \ref{fig: n4-synthetic}, where FedAvg completely fails by reaching only $58.4\pm2.33\%$, whereas our \texttt{FedPeWS} approach reaches $91.13\pm3.55\%$ accuracy by significantly outperforming the base optimizer (FedAvg) with a gain of $\mathbf{32.72\%}$. It is crucial to highlight that in this experiment, we set $\lambda=0.0$, effectively not enforcing diversity as outlined in Equation \ref{eq: mask-objective-func}. This approach focuses solely on optimizing the masks using the first loss component, which depends only on the data distributions of each participant. This shows that, in specific scenarios, we can learn the personalized masks (Procedure I) without the need to adjust the $\lambda$ parameter, while still achieving a better performance than the base optimizer.


\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.495\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/cifarmnist-both-last-v2.pdf}
        \caption{CIFAR-MNIST dataset.}
        \label{fig: cifar-mnist-figure-heatmap}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.495\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/pot-both-last-v2.pdf}
        \caption{\{Path-OCT-Tissue\}MNIST dataset.}
        \label{fig: pot-figure-heatmap}
    \end{subfigure}
    \caption{Results for experiments on (a) the CIFAR-MNIST and (b) \{Path-OCT-Tissue\}MNIST datasets with a communication budget of $T=300$. \textbf{(a) Left:} Participant 1 has MNIST data samples; Participant 2 has CIFAR-10 data samples. \textbf{(a) Right:} Ablation study for $\lambda$ and $\tau$ parameters on CIFAR-MNIST (see Table \ref{tab: cifar-mnist-ablation}). \textbf{(b) Left:} Each of $N=3$ participants holds unique dataset samples from \{PathMNIST, OCTMNIST, TissueMNIST\} pool. \textbf{(b) Right:} Ablation study for $\lambda$ and $\tau$ on the respective dataset (see Table \ref{tab: pot-ablation}). The first column ($\tau=0.0$) corresponds to the FedAvg algorithm. The last row presents results for the \texttt{FedPeWS-Fixed} algorithm.}
    \label{fig: cifar-mnist-pot-accuracy-heatmap} 
\end{figure}

\begin{figure}
    \centering
    {\includegraphics[trim={0cm 0.2cm 0 0.2cm},clip,width=\linewidth]{images/dir-cifar-mnist-n10-zoom-partial-v2.pdf}}
    \caption{Top: illustration of number of samples per class allocated to each client, that is indicated by dot sizes, for different concentration $\alpha$ values. Bottom: visualization of the experiments on CIFAR-MNIST dataset with $N=10$ participants with different levels of heterogeneity.} 
    \label{fig: dirichlet}
\end{figure}


\paragraph{Sensitivity to $\lambda$ and $\tau$ parameters.} \Figref{fig: cifar-mnist-pot-accuracy-heatmap} showcases the results of experiments on the CIFAR-MNIST dataset with $N=2$ participants and \{Path-OCT-Tissue\}MNIST dataset with $N=3$ participants. 
The left-side plots of Figures \ref{fig: cifar-mnist-figure-heatmap} and \ref{fig: pot-figure-heatmap}, which depict the performance of the global model (averaged over 3 runs), demonstrate that our method consistently achieves higher accuracy. The right side figures feature heatmap plots that annotate the global model accuracy obtained varying $\lambda \in \{0, 0.1, 0.3, 0.5, 1, 2, 5, 10, 100, 1000\}$ and $\tau \in \{0.0, 0.2, 0.4, 0.5, 0.6, 0.8\}$ parameters. An additional row labeled $(\lambda=-)$ represents the \texttt{FedPeWS-Fixed} approach, where user(server)-defined fixed masks are employed. In this method, we simply split the full network into $N$ partitions, with each partition assigned to a participant (for detailed instructions on setting masks, please see Section \ref{subsection: fixed-mask-how-to} in the appendix). The results indicate that our approach has a low sensitivity to variations in $\lambda$ and $\tau$. For more detailed insights, please refer to Tables \ref{tab: cifar-mnist-ablation} and \ref{tab: pot-ablation} in the appendix. 


\paragraph{Varying degrees of heterogeneity.} \Figref{fig: dirichlet} demonstrates that our \texttt{FedPeWS} approach consistently outperforms FedAvg, with gains directly related to the degree of data heterogeneity. The figure clearly shows that the advantage of using our method is more pronounced under conditions of high data heterogeneity. As heterogeneity levels decrease, our method becomes comparable to FedAvg. 

\input{table-synth-fedprox} 

\begin{figure}[t]
    \centering
    \includegraphics[trim={0cm 0.2cm 0 0.2cm},clip,width=\linewidth]{images/prox-comb-synth-4-line-v2.pdf} 
    \caption{Comparison of our proposed method and FedProx \citep{li2020federated} on Synthetic-\{32, 3.2\}K datasets. Refer to Table \ref{table: convergence-table-fedprox} for the corresponding numbers.} 
    \label{fig:fedprox-scenario13}
\end{figure}

\paragraph{FedProx.} We also present results using the FedProx optimizer on Synthetic-32K and Synthetic-3.2K datasets in Figure \ref{fig:fedprox-scenario13} and Table \ref{table: convergence-table-fedprox}, employing global learning rates $\eta_g = \{1.0, 0.5, 0.25, 0.1\}$. Note that we adapt Algorithm \ref{algorithm: adaptivesubnet} to incorporate the FedProx algorithm as the base optimizer, instead of FedAvg. 
We selected the best performing proximal term scaler $0.01$ after tuning and evaluating different values from a set of potential values $\{0.001, 0.01, 0.1, 0.5\}$, based on the findings in \citep{li2020federated}. The results demonstrate that \texttt{FedPeWS} outperforms FedProx in terms of both communication efficiency and final accuracy across the tested scenarios, except the last scenario (Synthetic-3.2K dataset with batch size 8 and $\eta_g=0.1$), where the performance of \texttt{FedPeWS} is comparable to that of FedProx. 


\section{Conclusion} 
In this work, we introduced a novel concept called \textit{personalized warmup via subnetworks} for heterogeneous FL $-$ a strategy that enhances convergence speed and can seamlessly integrate with existing optimization techniques. Results demonstrate that the proposed \texttt{FedPeWS} approach and achieves higher accuracy than the relevant baselines, especially when there is extreme statistical heterogeneity. Limitations of \texttt{FedPeWS} include the need to tune two additional hyperparameters (no. of warmup rounds and mask diversity weight) and the lack of theoretical convergence analysis. 


\bibliographystyle{plain}
\bibliography{main}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix 
\section{Related Work}

Table \ref{tab: methods-comparison} shows the comparison of our proposed approach to the existing literature. 

\input{table-comparison} 

\section{Additional Experimental Details} 
\label{suppl-section: arch} 

\subsection{Network Architecture Details} 
The network for the synthetic dataset (detailed in Section \ref{subsection: exp-setup}) consists of five fully connected (FC) layers, each followed by  ReLU activation functions, except for the last layer. We provide the details of this architecture in Table \ref{tab: synthetic-arch}. 

\begin{table}[th]
    \centering
    \caption{Architecture for synthetic dataset models used in the experiments.}
    \resizebox{0.75\linewidth}{!}{ 
        \begin{tabular}{lllllll} 
            \toprule 
            \cellcolor{lightblue}\textbf{Layer} & Input & FC1 & FC2 & FC3 & FC4 & FC5 \\ \midrule 
            \cellcolor{lightblue}\textbf{Dimensions} & [5] & [5, 32] & [32, 64] & [64, 128] & [128, 32] & [32, 4] \\ \bottomrule
        \end{tabular}
    }
    \label{tab: synthetic-arch}
\end{table}

The network for the CIFAR-MNIST and \{Path-OCT-Tissue\}MNIST datasets includes three convolutional layers followed by max pooling, and a fully connected layer. The details of this architecture is provided in Table \ref{tab: conv-arch}. 
\begin{table}[H]
    \centering
    \caption{Architecture for CIFAR-MNIST dataset models. Every convolutional layer is followed by a max pooling layer with kernel size 2 and stride 2. }
    \resizebox{\linewidth}{!}{ 
        \begin{tabular}{lllllll}
            \toprule
             \cellcolor{lightblue}\textbf{Layer} & Input & Conv1 & Conv2 & Conv3 & Flatten & FC \\ \midrule 
             \cellcolor{lightblue}\textbf{Dimensions} & [3, 32, 32] & [3, 32, 3, 3] & [32, 64, 3, 3] & [64, 128, 3, 3] & [2048] & [2048, 20] \\ \bottomrule 
        \end{tabular}
    }
    \label{tab: conv-arch}
\end{table}


\subsection{Fixed Mask Generation} 
\label{subsection: fixed-mask-how-to}
\Figref{fig: fixed-mask-split} illustrates how we design masks for \texttt{FedPeWS-Fixed} experiments in scenarios with $N=2$ participants. For cases involving $N=4$ participants, the full network $\mathcal{M}_x$ (classifier $\mathcal{M}$ parameterized with $x$) is divided into four subnetworks, vertically, with each subnetwork corresponding to one of the participants. As such, we vertically partition the hidden neurons in the network into $N$ groups (subnetworks) and design the mask to assign each group to one participant, ensuring no overlap. This design choice is based on the assumption that classes held by each participant are highly heterogeneous, thus preventing any intersection in the masks. This setting is specifically tailored for the \texttt{FedPeWS-Fixed} method and doesn't necessitate performing optimization over the masks $m_i$, they are kept fixed.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/fixedmask_V1-1500.pdf}
    \caption{Illustration of manual mask setting in the \texttt{FedPeWS-Fixed} method. The left figure illustrates the complete network with all neurons active and full connections. The middle figure represents subnetwork 1, utilizing only the left portion of the full network, where $m_1$ corresponds to this left side. Conversely, the right figure indicates the part of the network used for subnetwork 2. This setting is employed in all experiments involving $N=2$ participants.} 
    \label{fig: fixed-mask-split}
\end{figure}


\section{Experimental Results}

\subsection{Wall-clock Time vs. Accuracy} 
Figure \ref{fig: wall-clock} illustrates the wall-clock time versus accuracy results, which correspond to Figure \ref{fig: synthetic-all} in the main paper. From this comparison, \texttt{FedPeWS} demonstrates a slightly improved performance over FedAvg in terms of wall-clock time in two of the four scenarios. However, it underperforms slightly in the remaining two scenarios, with only a marginal increase in time. This variance is attributed to the alternation between training masks and weights during the warm-up phase, impacting the time efficiency. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/wall-clock.pdf}
    \caption{Wall-clock time vs. accuracy plot corresponding to Figure \ref{fig: synthetic-all} of the main paper.}
    \label{fig: wall-clock}
\end{figure}

\subsection{\texttt{FedPeWS-Fixed}. Mask Length Study} 
In this section, we explore the impact of mask length on the performance of the \texttt{FedPeWS-Fixed} method with parameter $W=120 (\tau=0.4)$ using the CIFAR-MNIST dataset. We examine two scenarios for splitting the network into two subnetworks: 
\begin{enumerate}
    \item $|m_1| < |m_2|$: 75\% of the mask is assigned to Participant 2, 25\% to Participant 1. 
    \item $|m_1| = |m_2|$: equal sized masks are assigned to each participant. 
\end{enumerate}

\begin{figure}[ht] 
    \centering
    \includegraphics[width=0.75\linewidth]{images/mask-length-study-v2.pdf} 
    \caption{Mask length study using \texttt{FedPeWS-Fixed} method on CIFAR-MNIST dataset. }
    \label{fig: mask-length-study}
\end{figure} 

Figure \ref{fig: mask-length-study} displays the validation accuracy over $T=300$ communication rounds for both scenarios. The leftmost plot shows the accuracy of the global model, while the middle and rightmost plots the accuracy for each of the participants. Both mask length scenarios converge to a comparable accuracy levels, with a marginal difference of $0.5\%$ higher accuracy in the scenario where $|m_1| < |m_2|$. This is likely due to the larger mask size, which aids in learning the more complex CIFAR-10 dataset held by Participant 2. 


\subsection{Sensitivity Analysis} 
In this section, we detail the sensitivity analysis of the $\lambda$ and $\tau$ parameters conducted on the CIFAR-MNIST dataset (Table \ref{tab: cifar-mnist-ablation}) and the \{Path-OCT-Tissue\}MNIST dataset (Table \ref{tab: pot-ablation}). This analysis particularly includes the standard deviation of the accuracy achieved by the tested algorithms after $T$ communication rounds and over three independent evaluations. Results with the best performance are highlighted in green. 

\input{table-cifar-mnist} 
\input{table-path-oct-tissue-mnist} 

For the CIFAR-MNIST dataset, the preferred values of $\lambda$ that yield optimal outcomes range within $\{2.0, 5.0, 10.0\}$, and for $\tau$, the values are $\{0.4, 0.5\}$. A similar pattern is observed in the \{Path-OCT-Tissue\}MNIST dataset experiment, with a small difference, in which it shows a preference for fewer warmup rounds ($\tau \in \{0.2, 0.4\}$) and demonstrates optimal performance with the same set of $\lambda$ values. This consistency across different datasets indicates robustness in the parameter settings for achieving high accuracy.                                            


\subsection{Larger number of participants} 
Although our primary focus is on the cross-silo setting, we extend our study to include a large-scale scenario involving 200 participants on the CIFAR-MNIST dataset. We adopt a Dirichlet partition strategy with concentration parameter $\alpha=0.5$ and implement this scenario with a partial participation rate of $0.1$. The outcomes of this experiment, as depicted in Figure \ref{fig: cifar-mnist-n200}, indicate a superior performance compared to the conventional FedAvg algorithm, thereby further substantiating the validity and effectiveness of our proposed method. The parameters set for \texttt{FedPeWS} are: $\tau=25$ and $\lambda=0.5$. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth]{images/cifar-mnist-n200.pdf}
    \caption{Visualization of global model performance with $N=200$ participants with a partial participation rate of $0.1$. Smoothing is applied as a running average with a window size of 5. A learning rate scheduler is implemented at rounds $200$ and $400$ with a learning rate decay factor of $0.1$.} 
    \label{fig: cifar-mnist-n200}
\end{figure} 


\section{Neuron Activations} 
In this section, we examine the extent to which neurons in each layer are activated. Our study uses the Synthetic-32K dataset and the \texttt{FedPeWS-Fixed} method (with parameter $W = 50$). The vertical dashed line ($W=50$) indicates the point at which participants switch to using full masks. 

\begin{figure}[b]
    \centering
    \includegraphics[width=\linewidth]{images/neuron-activation-trainset.pdf}
    \caption{Neuron activation study on the Synthetic-32K dataset with a global learning rate $\eta_g=1.0$. The experiment uses the \texttt{FedPeWS-Fixed} method with parameter $W=50$, indicated by the vertical dashed line, marking the switch to full masks by each participant.} 
    \label{fig: neuron-activations-train}
\end{figure} 

Figure \ref{fig: neuron-activations-train} displays the neuron activations, measured as the sum of activations over a batch of samples randomly selected from each participant's dataset, over $T=250$ communication rounds. The top row shows the outcomes for Participant 1, and the bottom row shows the activations for Participant 2. Each column corresponds to different fully connected layers (FC1 to FC4) in the network. Observations are as follows: 
\begin{enumerate}
    \item Before switching $(t \leq W)$: for Participant $i$, subnetwork $i$ shows higher activation patterns in all given FC layers, $i \in [1,2]$, while the other subnetwork exhibits a minimal activation. 
    \item After switching to full mask $(t > W)$: (i) there is a noticeable increase in activations for both participants upon switching to using full masks, (ii) Participant 1 with its originally initialized subnetwork 1, shows a substantial increase in activations compared to subnetwork 2 across all layers. The same pattern is obesrved for Participant 2 with subnetwork 2. 
\end{enumerate}

These findings suggest that the personalized warmup strategy helps the network learn which paths to follow when specific data points are fed into the network. This supports the superiority of our method and corroborates the claims made in the main paper. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}